
### 1. Ensemble Classifiers
- Motivation
	- For all datasets, a single algorithm will not always produce the most accurate model
	- Lets say a model classifies a set of data points incorrectly, there could be another model that can classify these data points correctly
	- Combining these models will give a better accuracy
- Idea
	- Combine multiple weak classifiers to obtain a strong classifier
	- What are weak classifiers?
		- Classifiers that perform just better than a random classifier
		- The random classifier will have a accuracy of 0.5 for 2-class classification problem
#### 1.1 What weak classifiers to choose? 
- We need each model to classify a subset of points correctly, using the same algorithm with same parameters is not useful
- To obtain multiple models: 
	- Choose different algorithms 
	- Choose different hyperparameters for the same algorithm
	- Use different input representations
	- Different training sets
		- Bootstrapping dataset
			- Choose a subset of data randomly and create multiple such subsets
			- Each of this subset will be a new dataset 
		- Use incorrectly classified points as the dataset to another model (used in boosting)
- Combining multiple output using voting
	- Combine the output of multiple high variance and low bias models
	- Averaging the output reduces the variance and bias remains small

### 2. Bagging (Bootstrap Aggregation)
- Bootstrap dataset into multiple subsets
- Train multiple models parallelly on each subset
- To obtain the prediction on test data, combine the predicted output of all the models using voting 
- Random forests: 
	- Decide on number of decision tree  
	- Train each decision tree to maximum depth on a particular subset obtained from bootstrapping
		- While choosing the attribute to split, choose the best attribute from a random subset of attributes 
	- We can limit the depth of the decision tree, this will be the hyperparameter
- In bagging each model has equal weight, the final voting is done by obtaining the label with max count

### 3. Boosting
- Method
	- Train a model on the dataset
	- The second model is trained on the data points that the first model trained incorrectly
#### 3.1 AdaBoost
- Training
	- Select the number of models: T
	- Each data point in the dataset has equal weight
		- Each label y is {+1, -1}
	- For T iterations
		- Train model to minimize the weighted error for each data point
			- Error (e)= $\sum_{\forall datapoints}\text{weight of datapoint} * \text{Error for datapoint prediction}$
		- If the model error is greater than 0.5 discard the model
		- Find the weight of the model: 
			- alpha = 0.5 * ln((1-e)/e) 
			- alpha is small if error is high
		- Update the weight of the data points
			- Weight of data point \*= e<sup>alpha * y<sub>true</sub> * y<sub>true</sub></sup>
			- Weight of data point will increase if y<sub>true</sub> != y<sub>pred</sub> and weight of data point decreases if the point is classified correctly 
- Prediction 
	- take the sign of $\sum_{\text{for all models}}y_{pred} * \alpha_{model}$
#### 3.2 Gradient boosting 
- XG boost: popular implementation of gradient boosting 
- Algorithms used in boosting: 
	- Decision trees
- Approach
	- First model will predict a value 
	- The second model will predict (y<sub>true</sub> - y<sub>pred of prev model</sub>)
	- The third model will predict (y<sub>true</sub> - y<sub>pred of prev model</sub>)
- Training
	- Initialize model f0(x), f0(x)=average(y), the loss of the model is L(y<sub>i</sub>, f0(x<sub>i</sub>))
	- Repeat for M iterations
		- For all the data points find $\partial{L(y, f0(x))}/\partial{f0(x)}$ which is called the **residual**
			- To understand how this give the difference between predicted and true value, use squared error and differentiate it wrt y<sub>pred</sub>
		- The next model will be trained the residual as the true labels h(x) 
		- To find the weight of the model find $\gamma$ that minimizes L(y, f0(x) + $\gamma$ * h(x))
		- Update the model: f1 = f0(x) + $\gamma$ * h(x)
	- end Repeat

### 4. Stacking
- Approach
	- Train multiple models on different subsets of the data (component classifiers)
	- Train a combiner classifier with the input as previous models output and the predicted output is the true label 
	- The combiner classifier is trained on the data that is not used to train the models
	- Retrain component classifiers of the entire dataset 
- In voting, all the models have equal weight. Stacking was proposed to learn the model weights rather that having all models have equal weight