

## 1. Naive Bayes classifier
- Transductive, generative and non parametric model
- Classes are predicted based on the probability that the give feature can generate that class as output
- The Naive Bayes classifier uses the Bayes theorem to predict the output class given the input
	- p(y|x) = (p(x|y) * p(y)) / p(x)
	- Here y is the output and x is the input
	- p(y) = prior probability, probability of that class irrespective of the input x
	- p(x|y) = class likelihood
	- p(x) = marginal probability, what is the probability that the given x is seen irrespective of the y
- The probability of a given class y<sub>i</sub> given the input
	- P(y<sub>i</sub> | x) = 
		(P(x | y<sub>i</sub>) * p(y<sub>i</sub>))/(Î£(k = 1 to N)P(x | y<sub>k</sub>)\*P(y<sub>k</sub>))
	- For a particular test data point all the classes will have the same denominator, this denominator can be ignored 
	- Maximum likelihood hypothesis
		- When we assume that each class is equally likely
		- This implies that the prior probability of all classes are equal
- Why is this called Naive Bayes?
	- To calculate the likelihood, we assume that each feature is independent
	- p(x | y<sub>i</sub>) = product of t = 1 to m, p(x<sub>k</sub> | y<sub>i</sub>), where m is the number of features in the dataset