### 1 Asymptotic notations
- **What is input size?**
	- Size of the input is problem dependent
	- Example:
		- For an array, the input size is the number of elements in the array
		- For multiplication of two numbers, the input size is the number of bits used to represent each number
- **Running time of an algorithm?**
	- Represent the running time in terms of the number of times the instruction is executed multiplied by the time taken to execute the instruction 
	- It is assumed that the time taken to execute an instruction is constant
	- This leaves us with the number of times the instruction is executed which depends on the input size
- **Asymptotic Notation**
	- Represents how the running time increases with the increase in input size
		- For example, finding if a number is odd or even is constant time no matter what the size of input is (using the last bit of the number)
		- Searching for an element in a list of element, the running time increases linearly with the increase in input size
		- Finding all the permutations of a list of unique numbers: running time will be factorial of the input size
#### 1.1 Theta notation: Θ
- A function f(n) belongs to a family of functions Θ(g(n)) if there exists positive constants c<sub>1</sub>, c<sub>2</sub>, n<sub>0</sub> such that
	- 0 <= c<sub>1</sub>g(n) <= f(n) <= c<sub>2</sub>g(n) for all n >= n<sub>0</sub>
- When f(n) belongs to Θ(g(n)): 
	- Implies that f(n) belongs to both O(g(n)) and Ω(g(n))
	- The upper bound of the function f is denoted by Θ but the lower bound need not be in the same boundary
		- This is evident because, the inequality is true after a particular input size n<sub>0</sub>, the function f can have a best case value for the input size less than n<sub>0</sub> 
		- For example the best case for insertion sort is one iteration, and the worst case is n<sup>2</sup> iterations, the insertion sort has a time complexity of Θ(n<sup>2</sup>), but this does not cover the minimum runtime of the insertion sort
#### 1.2 Big O notation: O
- A function f(n) belongs to a family of functions O(g(n)) if there exists positive constants c, n<sub>0</sub> such that
	- 0 <=  f(n) <= cg(n) for all n >= n<sub>0</sub>
- Using Θ is more asymptotically tight, i.e. This describes the bounds of a function better than big O notation
- Using Big O only denotes the upper bound, this is not asymptotically tight always. Example 
	- Let the function f(n) belong to O(n). The function will also belong to O(n<sup>2</sup>). Both are correct, but the former is more asymptotically tight upper bound
#### 1.3 Big Omega notation: Ω
- A function f(n) belongs to a family of functions Ω(g(n)) if there exists positive constants c, n<sub>0</sub> such that
	- 0 <= cg(n) <= f(n) for all n >= n<sub>0</sub>
#### 1.4 Small o notation: o
- Used to denote the upper bound that is not asymptotically tight
- A function f(n) belongs to a family of functions o(g(n)) if there exists positive constant n<sub>0</sub> for all positive constant c > 0, the following inequality holds true
	- 0 <= f(n) < cg(n) for all n >= n<sub>0</sub> 
#### 1.5 Small omega notation: Ω 
- Used to denote the lower bound that is not asymptotically tight
- A function f(n) belongs to a family of functions Ω(g(n)) if there exists positive constant n<sub>0</sub> for all positive constant c > 0, the following inequality holds true
	- 0 <= cg(n) < f(n) for all n >= n<sub>0</sub> 

### 2. Average case analysis for Quick sort
- To perform the average case analysis, consider the key operation as comparing two elements
- E(X) = expectation of two elements being compared
- E(X) = E(ΣX<sub>ij</sub> ( for 1 <= i <= N-1 and j > i))
	- where X<sub>ij</sub> is a random var that equals 1 when the i<sup>th</sup> and j<sup>th</sup> elements in sorted order are compared and 0 other wise
- E(X) = ΣE(X<sub>ij</sub> ( for 1 <= i <= N-1 and j > i))
- E(X<sub>ij</sub>) = 2/(j - i + 1)
	- i<sup>th</sup> and j<sup>th</sup> are compare only when either one of the elements are picked as pivot, they are not compared when any element between them is picked.
	- When any element between them is picked as pivot, both the element will belong to different sub trees and will not be compared
- E(X) will belong to Θ(N\*log(N)), this can be obtained by expanding the summation, we will get a sum of a harmonic progression multiplied by n, which is n\*lg(n)

### 3. Analyzing Randomized algorithms
#### 3.1 Polynomial Identity Testing
- Given two polynomials f(x) and g(x), both polynomial are the same if the domain and the range are equal, and for each input in domain, both polynomial output the same value
- Let f(x) and g(x) be polynomials of a maximum degree d
- The algorithm to find if they are equal is as follows
```
1. Begin Polynomial Identity Testing (f(x), g(x), d)
2. Pick a random number r from [0, 1000d]
3. if f(r) is equal to g(r), then polynomials are identical
4. else not identical
5. End
```
- Analyzing the algorithm
	- Case 1: f(x) and g(x) are equal
		- For this case the algorithm always outputs the correct value
	- Case 2: f(x) is not equal to g(x)
		- Correct answer when r is not root of f(x) and g(x)
		- Wrong answer when r is the root of f(x) and g(x)
			- P(wrong answer) <= number of roots/(range in which the numbers are picked in random)
			- The worst case is when all the roots of the polynomial are in the range we picked
			- P(wrong answer) <= d/1000d 
- Improving the algorithm
	- Range in which we pick the number is directly proportional to P(correct answer)
	- Another option is to repeat the algorithm multiple times, i.e. pick multiple r and check for the result
		- If the answer is not identical in any one case, we can output that polynomials are not identical
	- Let's say we pick the a random number 1000 times, P(wrong answers for all 1000 cases) = p(wrong answer)<sup>1000</sup>
	- How to choose the number of iterations?
		- The number of iterations = d+1, and we pick each number in the range without replacement
		- why d+1? the polynomial at most can have d roots, lets say we picked all d roots, the d+1th turn we are guaranteed to pick an element that is not a root(only if we pick without replacing)
		- Sampling with replacement is still useful because the implementation is simpler
#### 3.2 Min Cut
- Given a graph G, the min cut is the minimum number of edges to remove to make the graph into to disjoint subgraphs
- Begin Find Min Cut Size G(V:vertices , E:edges)
	- Pick an edge in random form E
	- Combine the nodes that has the edge, i.e. delete the edge and combine the two nodes that had the edge into a single node
	- All self loops can be ignored, if there are two edges between two nodes, both edges should be preserved (unless one of them was picked for contraction)
	- Repeat until only two vertices remain
	- The number of edges between the two vertices is the size of the min cut
- Analysis
	- This algorithm give the correct answer with a certain probability
	- The answer will be incorrect when the min cut edge is deleted
	- Let's say the size of the min cut is k, each vertex should have a minimum degree of k
		- Why? if degree of that node is less than k, then that will be the size of the min cut
	- Therefore there will be **at least** nk/2 edges, where n is the number of vertices
		- Why $n*k/2$?: Each node will have k edges, the total edges will be $n*k$. For each pair of nodes that has an edge, we counted the edge twice, once for each node. So the total number of edges without duplicates will be $n*k/2$
	- P(choosing the min cut edge) <= k/(nk/2) = 2/n
	- P(choosing the min cut edge for second edge) <= 2/n-1
	- P(min cut edge surviving) >= 1 - 2/n
	- P(min cut edge surviving after n-2 cuts) >= product(1<=j<=n-2) {1 - 2/(n-j+1)} = 1/n<sup>2</sup>
		- Why n-2 cuts? In each cut we are reducing the number of nodes by 1, we stop when there are only two vertices, therefore n-2 cut
		- How is this $1/n^2$? Expand the summation and take lcm, most of the numerators and denominators get cancelled(telescoping series)
	- P(wrong min cut size) >= 1 - 1/n<sup>2</sup>
	- This probability can be reduced by performing independent trails multiple times and taking the minimum of all those trails

#### 4. Galactic Algorithms
- Algorithms that have huge theoretical impact but is not practical due to its complexity or large values of inputs where the difference is evident
#### 4.1 Primality testing
- Check if a give number is prime. The usual approach is
	- Check if N is divisible by any number in range \[2, sqrt(N)]
	- Factors considered in complexity of this algorithm
		- Finding sqrt(N): 
			- There are polynomial time algorithms for calculating this #TODO-EXTRA
		- Finding if N is divisible by a number
			- There are polynomial time algorithms to find number divisibility
		- How many time the division is repeated:
			- In terms of N, this algorithm will use run for sqrt(N), but we'll need to consider the bit-wise representation of the number
			- N = 2<sup>b</sup> bits
			- sqrt(N) = 2<sup>b/2</sup> : which is exponential time algorithm in terms of the number of bits to represent the digits
- Polynomial time algorithm: AKS primality testing
	- This is a deterministic, general and polynomial time algorithm 
	- This is a galactic algorithm, the benefits of the algorithm is visible only when N is very large (so large that these values of N are not practically used)

